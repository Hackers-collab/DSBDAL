{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b69311-c1f3-45e9-b1c7-9477cc95a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anuj/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/anuj/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/anuj/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/anuj/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /Users/anuj/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d2e92-e6d4-497d-9065-5c51709afb45",
   "metadata": {},
   "source": [
    "## **1.Sentence Tokenization And Word Tokenization** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05df669-8b76-47be-a26d-081a39114416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Hello', 'my', 'name', 'is', 'anuj', '.', 'And', 'i', \"'m\", 'currently', 'pursuig', 'BE', 'Computer', 'Degree', 'from', 'MESWCOE', '.']\n",
      "Sentence Tokenization: ['Hello my name is anuj.', \"And i'm currently pursuig BE Computer Degree from MESWCOE.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Hello my name is anuj. And i'm currently pursuig BE Computer Degree from MESWCOE.\"\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Word Tokenization:\", words)\n",
    "print(\"Sentence Tokenization:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3a282-b6d9-4967-a9eb-66c4f4586f52",
   "metadata": {},
   "source": [
    "## **2.Stop Words** ##\n",
    "- Contain a common list of words\n",
    "- With the help of this list we can remove the unimportant words(stop words) from the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "554fb95e-9bfb-4784-a818-cf9f37f10401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize Sentence: ['Hello', 'my', 'name', 'is', 'anuj', 'And', 'i', 'm', 'currently', 'pursuing', 'BE', 'Computer', 'Degree', 'from', 'MESWCOE', 'college']\n",
      "Filtered Sentence: ['Hello', 'name', 'anuj', 'And', 'currently', 'pursuing', 'BE', 'Computer', 'Degree', 'MESWCOE', 'college']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Load the common words and store it on the 'stop_words' variable\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# print(\"Stop Words:\", stop_words)\n",
    "\n",
    "text = \"Hello my name is anuj. And i'm currently pursuing BE Computer Degree from MESWCOE college\"\n",
    "\n",
    "# Remove anything that's not a letter and replaces it with a space\n",
    "text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "words = word_tokenize(text)\n",
    "filtered_text = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        filtered_text.append(word)\n",
    "\n",
    "print(\"Tokenize Sentence:\", words)\n",
    "print(\"Filtered Sentence:\", filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da4b48-a1bd-4cde-8054-c9ece1e0bf16",
   "metadata": {},
   "source": [
    "## **3.Stemming** ##\n",
    "- Help to reduced words to their root form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "242fba8d-1a9d-4208-b1b6-e6fc82edff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "demur\n",
      "jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"Run\", \"Demured\",\"Jumping\"]\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    rootWord = ps.stem(word)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db2184-afc8-481b-88b7-371abd8352ce",
   "metadata": {},
   "source": [
    "## **4.Lemmatization** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f286c80e-0e4f-4cf5-98e5-b783a1ff7a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies studying cries cry\n",
      "Lemma for studies is study \n",
      "Lemma for studying is studying \n",
      "Lemma for cries is cry \n",
      "Lemma for cry is cry \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "\n",
    "print(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {} \".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c99a4-dc9c-400b-af00-b94dfb3d42b7",
   "metadata": {},
   "source": [
    "## **5.POS Tagging** ##\n",
    "- Label words with their Part Of Speech (POS)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02839c74-46cc-485b-8acc-085a76154bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Anuj', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('learning', 'VBG')]\n",
      "[('AI', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('help', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('book.But', 'NN')]\n",
      "[('when', 'WRB')]\n",
      "[('he', 'PRP')]\n",
      "[('read', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('half', 'NN')]\n",
      "[('book', 'NN')]\n",
      "[('.', '.')]\n",
      "[('The', 'DT')]\n",
      "[('concept', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('book', 'NN')]\n",
      "[('baffled', 'VBN')]\n",
      "[('Him', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# uncomment this \n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "data = \"Anuj is learning AI with the help of the book. But when he read the half book. The concept of the book baffled Him\"\n",
    "words = word_tokenize(data)\n",
    "\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7584f07-916d-4aba-99b8-65f9e8204daa",
   "metadata": {},
   "source": [
    "## **6.Term Frequency(TF)** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8e9459b-5841-4100-b891-0b3399ed3fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: {'college', 'pune', 'of', 'best', 'in', 'WADIA', 'one', 'is', 'COE', 'the'}\n",
      "\n",
      "TF for Document A:\n",
      " {'WADIA': 0.1, 'COE': 0.1, 'is': 0.1, 'one': 0.1, 'of': 0.1, 'the': 0.1, 'best': 0.1, 'college': 0.1, 'in': 0.1, 'pune': 0.1}\n",
      "\n",
      "TF for Document B:\n",
      " {'WADIA': 0.14285714285714285, 'COE': 0.14285714285714285, 'is': 0.14285714285714285, 'one': 0.14285714285714285, 'of': 0.14285714285714285, 'the': 0.14285714285714285, 'best': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "docA = \"WADIA COE is one of the best college in pune\"\n",
    "docB = \"WADIA COE is one of the best\"\n",
    "\n",
    "# Split into words\n",
    "wordsA = docA.split()\n",
    "wordsB = docB.split()\n",
    "\n",
    "# Unique words from both\n",
    "uniqueWords = set(wordsA + wordsB)\n",
    "print(\"Unique words:\", uniqueWords)\n",
    "\n",
    "# Create word count dictionary\n",
    "def makeDict(words):\n",
    "    word_count = dict.fromkeys(words, 0)  # use uniqueWords here\n",
    "    for word in words:\n",
    "        word_count[word] += 1\n",
    "    return word_count\n",
    "\n",
    "# TF calculation\n",
    "def compute_TF(word_count, words):\n",
    "    tf = {}\n",
    "    total_words = len(words)\n",
    "    for word, count in word_count.items():\n",
    "        tf[word] = count / total_words\n",
    "    return tf\n",
    "\n",
    "# Calculate\n",
    "numOfWordsA = makeDict(wordsA)\n",
    "numOfWordsB = makeDict(wordsB)\n",
    "\n",
    "tfA = compute_TF(numOfWordsA, wordsA)\n",
    "tfB = compute_TF(numOfWordsB, wordsB)\n",
    "\n",
    "# Output\n",
    "print(\"\\nTF for Document A:\\n\", tfA)\n",
    "print(\"\\nTF for Document B:\\n\", tfB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18946008-ad4b-409e-96eb-87309ec71aa7",
   "metadata": {},
   "source": [
    "## **7.IDF** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b8e5f1a-3666-4f2f-9882-dfaa000fa8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WADIA': 0.0,\n",
       " 'COE': 0.0,\n",
       " 'is': 0.0,\n",
       " 'one': 0.0,\n",
       " 'of': 0.0,\n",
       " 'the': 0.0,\n",
       " 'best': 0.0,\n",
       " 'college': 0.6931471805599453,\n",
       " 'in': 0.6931471805599453,\n",
       " 'pune': 0.6931471805599453}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "\n",
    "  # Length of the document\n",
    "  N = len(documents)\n",
    "\n",
    "  # IDF dictionary\n",
    "  idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "\n",
    "  for document in documents:\n",
    "\n",
    "    for word, val in document.items():\n",
    "      if val > 0:\n",
    "        idfDict[word] += 1\n",
    "\n",
    "  for word, val in idfDict.items():\n",
    "    idfDict[word] = math.log(N / float(val))\n",
    "\n",
    "  return idfDict\n",
    "\n",
    "\n",
    "idf = computeIDF([numOfWordsA, numOfWordsB])\n",
    "idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
